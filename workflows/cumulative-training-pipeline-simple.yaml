# Simplified Cumulative Training Pipeline
# Clean workflow that processes multiple games and trains a cumulative model

main:
  params: [args]
  steps:
    - init:
        assign:
          - game_ids: ${args.game_ids}
          - project_id: "refined-circuit-474617-s8"
          - region: "us-central1"
          - timestamp: ${string(int(sys.now()))}
          - execution_dir: ${"cumulative-execution-" + timestamp}
          - use_flash: ${default(args.use_flash, true)}

    - log_start:
        call: sys.log
        args:
          data: '${"ðŸš€ Starting cumulative training for " + string(len(game_ids)) + " games"}'
          severity: INFO

    # Check which games need JSONL creation
    - check_games:
        assign:
          - games_needing_processing: []
        for:
          value: game_id
          in: ${game_ids}
          steps:
            - check_jsonl_exists:
                try:
                  call: http.get
                  args:
                    url: '${"https://storage.googleapis.com/storage/v1/b/uball-training-data/o?prefix=" + text.url_encode("games/" + game_id + "/video_training_")}'
                    auth:
                      type: OAuth2
                  result: check_result
                except:
                  assign:
                    - games_needing_processing: ${list.concat(games_needing_processing, [game_id])}
                  next: continue
            - process_check_result:
                switch:
                  - condition: ${"items" in check_result.body and len(check_result.body.items) > 0}
                    call: sys.log
                    args:
                      data: '${"âœ… Found JSONL for game: " + game_id}'
                      severity: INFO
                  - condition: true
                    steps:
                      - log_missing:
                          call: sys.log
                          args:
                            data: '${"âŒ Missing JSONL for game: " + game_id + " - will generate"}'
                            severity: INFO
                      - add_to_processing:
                          assign:
                            - games_needing_processing: ${list.concat(games_needing_processing, [game_id])}

    # Process games that need JSONL creation
    - process_missing_games:
        switch:
          - condition: ${len(games_needing_processing) > 0}
            for:
              value: game_id
              in: ${games_needing_processing}
              steps:
                - export_plays:
                    call: http.post
                    args:
                      url: '${"https://" + region + "-" + project_id + ".cloudfunctions.net/export-plays-cf"}'
                      headers:
                        Content-Type: "application/json"
                      body:
                        game_id: ${game_id}
                    result: export_result
                
                - extract_clips:
                    call: googleapis.run.v1.namespaces.jobs.create
                    args:
                      parent: '${"namespaces/" + project_id}'
                      location: ${region}
                      body:
                        apiVersion: run.googleapis.com/v1
                        kind: Job
                        metadata:
                          name: '${"extract-" + game_id + "-" + timestamp}'
                        spec:
                          template:
                            spec:
                              parallelism: 1
                              taskCount: 1
                              template:
                                spec:
                                  containers:
                                  - image: '${"gcr.io/" + project_id + "/extract-clips:latest"}'
                                    env:
                                    - name: GAME_ID
                                      value: ${game_id}
                                    - name: PLAYS_FILE_GCS
                                      value: ${export_result.body.files.all_plays}
                                    - name: GCS_TRAINING_BUCKET
                                      value: "uball-training-data"
                                    - name: GCS_VIDEO_BUCKET
                                      value: "uball-videos-production"
                                    resources:
                                      limits:
                                        memory: 16Gi
                                        cpu: "8"
                                  maxRetries: 3
                                  timeoutSeconds: "3600"
                    result: extract_job
                
                - run_extract_job:
                    call: googleapis.run.v1.namespaces.jobs.run
                    args:
                      name: '${"namespaces/" + project_id + "/jobs/" + extract_job.metadata.name}'
                      location: ${region}
                    result: extract_execution
                
                - wait_extract:
                    call: poll_until_complete
                    args:
                      execution_name: ${extract_execution.metadata.name}
                      project_id: ${project_id}
                      max_minutes: 60

    # Combine all JSONL files
    - combine_files:
        call: googleapis.run.v1.namespaces.jobs.create
        args:
          parent: '${"namespaces/" + project_id}'
          location: ${region}
          body:
            apiVersion: run.googleapis.com/v1
            kind: Job
            metadata:
              name: '${"combine-" + timestamp}'
            spec:
              template:
                spec:
                  parallelism: 1
                  taskCount: 1
                  template:
                    spec:
                      containers:
                      - image: "gcr.io/google.com/cloudsdktool/cloud-sdk:latest"
                        command: ["/bin/bash"]
                        args:
                          - "-c"
                          - |
                            echo "ðŸš€ Combining JSONL files for multiple games..."
                            
                            # Create execution directory
                            gsutil -m mkdir -p "gs://uball-training-data/${EXECUTION_DIR}/"
                            
                            # Initialize combined files
                            echo "" > /tmp/combined_training.jsonl
                            echo "" > /tmp/combined_validation.jsonl
                            
                            # Process each game
                            for GAME_ID in $GAME_ID_1 $GAME_ID_2; do
                              if [ -z "$GAME_ID" ]; then continue; fi
                              echo "ðŸ“‚ Processing game: $GAME_ID"
                              
                              # Copy game data to execution directory
                              gsutil -m mkdir -p "gs://uball-training-data/${EXECUTION_DIR}/${GAME_ID}/"
                              gsutil -m cp -r "gs://uball-training-data/games/${GAME_ID}/*" "gs://uball-training-data/${EXECUTION_DIR}/${GAME_ID}/" 2>/dev/null || true
                              
                              # Combine training files
                              gsutil ls "gs://uball-training-data/games/$GAME_ID/video_training_*.jsonl" | head -1 | xargs -I {} gsutil cp {} /tmp/game_training.jsonl 2>/dev/null || true
                              if [ -f /tmp/game_training.jsonl ]; then
                                cat /tmp/game_training.jsonl >> /tmp/combined_training.jsonl
                              fi
                              
                              # Combine validation files  
                              gsutil ls "gs://uball-training-data/games/$GAME_ID/video_validation_*.jsonl" | head -1 | xargs -I {} gsutil cp {} /tmp/game_validation.jsonl 2>/dev/null || true
                              if [ -f /tmp/game_validation.jsonl ]; then
                                cat /tmp/game_validation.jsonl >> /tmp/combined_validation.jsonl
                              fi
                            done
                            
                            # Upload combined files
                            gsutil cp /tmp/combined_training.jsonl "gs://uball-training-data/${EXECUTION_DIR}/combined_training.jsonl"
                            gsutil cp /tmp/combined_validation.jsonl "gs://uball-training-data/${EXECUTION_DIR}/combined_validation.jsonl"
                            
                            echo "âœ… Combined files ready!"
                        env:
                        - name: EXECUTION_DIR
                          value: ${execution_dir}
                        - name: GAME_ID_1
                          value: ${game_ids[0]}
                        - name: GAME_ID_2  
                          value: '${ len(game_ids) > 1 ? game_ids[1] : "" }'
                        resources:
                          limits:
                            memory: 4Gi
                            cpu: "2"
                      maxRetries: 3
                      timeoutSeconds: "1800"
        result: combine_job

    - run_combine_job:
        call: googleapis.run.v1.namespaces.jobs.run
        args:
          name: '${"namespaces/" + project_id + "/jobs/" + combine_job.metadata.name}'
          location: ${region}
        result: combine_execution

    - wait_combine:
        call: poll_until_complete
        args:
          execution_name: ${combine_execution.metadata.name}
          project_id: ${project_id}
          max_minutes: 30

    # Start Vertex AI tuning
    - start_tuning:
        assign:
          - base_model: '${ use_flash ? "gemini-2.5-flash" : "gemini-2.5-pro" }'
          - model_prefix: '${ use_flash ? "basketball-flash" : "basketball-pro" }'
          - training_file: '${"gs://uball-training-data/" + execution_dir + "/combined_training.jsonl"}'
          - validation_file: '${"gs://uball-training-data/" + execution_dir + "/combined_validation.jsonl"}'

    - create_tuning_job:
        call: http.post
        args:
          url: '${"https://" + region + "-aiplatform.googleapis.com/v1/projects/" + project_id + "/locations/" + region + "/tuningJobs"}'
          auth:
            type: OAuth2
          headers:
            Content-Type: "application/json"
          body:
            baseModel: ${base_model}
            supervisedTuningSpec:
              trainingDatasetUri: ${training_file}
              validationDatasetUri: ${validation_file}
              hyperParameters:
                epochCount: "5"
                learningRateMultiplier: "1.0"
                adapterSize: "ADAPTER_SIZE_ONE"
            tunedModelDisplayName: '${"" + model_prefix + "-cumulative-" + string(len(game_ids)) + "games-" + timestamp}'
            description: '${"Cumulative training on " + string(len(game_ids)) + " games using " + base_model}'
        result: tuning_response

    - log_tuning_started:
        call: sys.log
        args:
          data: '${"ðŸ¤– Started tuning job: " + tuning_response.body.name}'
          severity: INFO

    # Monitor tuning completion
    - wait_tuning:
        call: poll_tuning_job
        args:
          job_name: ${tuning_response.body.name}
          max_minutes: 480

    - log_success:
        call: sys.log
        args:
          data: '${"ðŸŽ‰ Cumulative training completed for " + string(len(game_ids)) + " games!"}'
          severity: INFO

    - return_result:
        return:
          success: true
          games_trained: ${game_ids}
          total_games: ${len(game_ids)}
          training_file: ${training_file}
          validation_file: ${validation_file}
          execution_directory: ${execution_dir}

# Simplified polling subworkflow
poll_until_complete:
  params: [execution_name, project_id, max_minutes]
  steps:
    - poll_loop:
        assign:
          - iterations: 0
          - max_iter: ${max_minutes * 2}  # 30s intervals
    - check_status:
        call: googleapis.run.v1.namespaces.executions.get
        args:
          name: '${"namespaces/" + project_id + "/executions/" + execution_name}'
          location: us-central1
        result: status
    - evaluate_status:
        switch:
          - condition: ${len(status.status.conditions) > 0 and status.status.conditions[0].type == "Completed"}
            return: ${status}
          - condition: ${len(status.status.conditions) > 0 and status.status.conditions[0].type == "Failed"}
            raise: '${"Execution failed: " + status.status.conditions[0].message}'
          - condition: ${iterations >= max_iter}
            raise: '${"Execution timed out after " + string(max_minutes) + " minutes"}'
    - wait_and_retry:
        call: sys.sleep
        args:
          seconds: 30
    - increment_and_continue:
        assign:
          - iterations: ${iterations + 1}
        next: check_status

# Simplified tuning job polling
poll_tuning_job:
  params: [job_name, max_minutes]
  steps:
    - poll_loop:
        assign:
          - iterations: 0
          - max_iter: ${max_minutes}  # 1 minute intervals
    - check_tuning:
        call: http.get
        args:
          url: '${"https://us-central1-aiplatform.googleapis.com/v1/" + job_name}'
          auth:
            type: OAuth2
        result: tuning_status
    - evaluate_tuning:
        switch:
          - condition: ${tuning_status.body.state == "JOB_STATE_SUCCEEDED"}
            return:
              success: true
              tuned_model: ${tuning_status.body.tunedModel.model}
          - condition: ${tuning_status.body.state == "JOB_STATE_FAILED"}
            raise: '${"Tuning failed: " + tuning_status.body.error.message}'
          - condition: ${iterations >= max_iter}
            raise: '${"Tuning timed out after " + string(max_minutes) + " minutes"}'
    - wait_and_retry:
        call: sys.sleep
        args:
          seconds: 60
    - increment_and_continue:
        assign:
          - iterations: ${iterations + 1}
        next: check_tuning