# Hybrid Training Pipeline - Cloud Functions + Cloud Run Jobs
# This orchestrates the basketball annotation training pipeline using:
# 1. Cloud Function for fast database export
# 2. Cloud Run Jobs for heavy video/ML processing

main:
  params: [args]
  steps:
    - init:
        assign:
          - game_id: ${args.game_id}
          - project_id: "refined-circuit-474617-s8"
          - region: "us-central1"
          - timestamp: ${string(int(sys.now()))}
          - pipeline_id: ${"hybrid-pipeline-" + game_id + "-" + timestamp}

    - log_start:
        call: sys.log
        args:
          data: '${"üöÄ Starting hybrid training pipeline for game: " + game_id}'
          severity: INFO

    # Step 1: Cloud Function - Export plays from database
    - export_plays:
        call: http.post
        args:
          url: '${"https://" + region + "-" + project_id + ".cloudfunctions.net/export-plays-cf"}'
          headers:
            Content-Type: "application/json"
          body:
            game_id: ${game_id}
        result: export_result

    - check_export:
        switch:
          - condition: ${export_result.body.success}
            next: log_export_success
          - condition: true
            next: export_failed

    - log_export_success:
        call: sys.log
        args:
          data: '${"‚úÖ Export completed: " + string(export_result.body.total_plays) + " plays exported"}'
          severity: INFO

    # Step 2: Cloud Run Job - Extract video clips
    - extract_clips_job:
        call: googleapis.run.v1.namespaces.jobs.create
        args:
          parent: '${"namespaces/" + project_id}'
          location: ${region}
          body:
            apiVersion: run.googleapis.com/v1
            kind: Job
            metadata:
              name: '${"extract-clips-" + game_id + "-" + timestamp}'
            spec:
              template:
                spec:
                  parallelism: 1
                  taskCount: 1
                  template:
                    spec:
                      containers:
                      - image: '${"gcr.io/" + project_id + "/extract-clips:latest"}'
                        env:
                        - name: GAME_ID
                          value: ${game_id}
                        - name: PLAYS_FILE_GCS
                          value: ${export_result.body.files.all_plays}
                        - name: GCS_TRAINING_BUCKET
                          value: "uball-training-data"
                        - name: GCS_VIDEO_BUCKET
                          value: "uball-videos-production"
                        resources:
                          limits:
                            memory: 16Gi
                            cpu: "8"
                      maxRetries: 3
                      timeoutSeconds: "86400"
        result: extract_job

    - log_extract_job_created:
        call: sys.log
        args:
          data: '${"üé¨ Created clip extraction job: " + extract_job.metadata.name}'
          severity: INFO

    # Execute the job
    - execute_extract_job:
        call: googleapis.run.v1.namespaces.jobs.run
        args:
          name: '${"namespaces/" + project_id + "/jobs/" + extract_job.metadata.name}'
          location: ${region}
        result: extract_execution

    - log_extract_job_started:
        call: sys.log
        args:
          data: '${"üöÄ Executing clip extraction job: " + extract_execution.metadata.name}'
          severity: INFO

    # Wait for clip extraction completion
    - wait_extract_completion:
        call: wait_for_execution_completion
        args:
          execution_name: ${extract_execution.metadata.name}
          project_id: ${project_id}
          timeout_minutes: 60  # 1 hour should be enough for clip extraction
        result: extract_status

    - log_extract_completion:
        call: sys.log
        args:
          data: '${"‚úÖ Clip extraction completed successfully"}'
          severity: INFO

    # Step 3: Format training data (could be another job, but keeping it simple for now)
    - format_training_data:
        call: format_data_step
        args:
          game_id: ${game_id}
          project_id: ${project_id}
        result: format_result

    - log_format_completion:
        call: sys.log
        args:
          data: '${"‚úÖ Training data formatted successfully"}'
          severity: INFO

    # Step 4: Direct Vertex AI Model Fine-tuning
    - create_tuning_job:
        call: http.post
        args:
          url: '${"https://" + region + "-aiplatform.googleapis.com/v1/projects/" + project_id + "/locations/" + region + "/tuningJobs"}'
          auth:
            type: OAuth2
          headers:
            Content-Type: "application/json"
          body:
            displayName: '${"basketball-model-" + game_id + "-" + timestamp}'
            baseModel: "gemini-1.5-flash-001"
            supervisedTuningSpec:
              trainingDatasetUri: "gs://uball-training-data/datasets/training_d6ba2cbb-da84-4614-82fc-ff58ba12d5ab_20251020_052353.jsonl"
              validationDatasetUri: "gs://uball-training-data/datasets/validation_d6ba2cbb-da84-4614-82fc-ff58ba12d5ab_20251020_052353.jsonl"
              hyperParameters:
                epochCount: "5"
                learningRateMultiplier: "1.0"
                adapterSize: "ADAPTER_SIZE_EIGHT"
            tunedModelDisplayName: '${"basketball-model-" + game_id + "-" + timestamp}'
        result: tuning_job_response

    - log_tuning_job_created:
        call: sys.log
        args:
          data: '${"ü§ñ Created Vertex AI tuning job: " + tuning_job_response.body.name}'
          severity: INFO

    # Monitor tuning job progress
    - wait_tuning_completion:
        call: monitor_tuning_job
        args:
          job_name: ${tuning_job_response.body.name}
          project_id: ${project_id}
          region: ${region}
          timeout_minutes: 480  # 8 hours for training
        result: tuning_status

    - log_tuning_completion:
        call: sys.log
        args:
          data: '${"‚úÖ Vertex AI model fine-tuning completed successfully"}'
          severity: INFO

    # Success completion
    - log_pipeline_success:
        call: sys.log
        args:
          data: '${"üéâ Hybrid training pipeline completed successfully for game: " + game_id}'
          severity: INFO

    - return_success:
        return:
          success: true
          pipeline_id: ${pipeline_id}
          game_id: ${game_id}
          export_result: ${export_result.body}
          extract_job_name: ${extract_job.metadata.name}
          tuning_job_name: ${tuning_job_response.body.name}
          tuned_model: ${tuning_status.tuned_model}
          vertex_ai_console: '${"https://console.cloud.google.com/vertex-ai/models?project=" + project_id}'
          message: "Hybrid training pipeline completed successfully with Vertex AI fine-tuning"

    # Error handling
    - export_failed:
        call: sys.log
        args:
          data: '${"‚ùå Export failed: " + string(export_result.body.error)}'
          severity: ERROR
        next: return_failure

    - return_failure:
        return:
          success: false
          pipeline_id: ${pipeline_id}
          game_id: ${game_id}
          error: "Pipeline failed"
          message: "Hybrid training pipeline failed"

# Subworkflow to wait for Cloud Run Job Execution completion
wait_for_execution_completion:
  params: [execution_name, project_id, timeout_minutes]
  steps:
    - init_polling:
        assign:
          - poll_interval_seconds: 30
          - max_iterations: ${timeout_minutes * 2}  # 30s intervals
          - iteration: 0

    - poll_execution_status:
        call: googleapis.run.v1.namespaces.executions.get
        args:
          name: '${"namespaces/" + project_id + "/executions/" + execution_name}'
          location: us-central1
        result: execution_status

    - check_execution_completion:
        switch:
          - condition: ${len(execution_status.status.conditions) > 0 and execution_status.status.conditions[0].type == "Completed"}
            next: execution_completed
          - condition: ${len(execution_status.status.conditions) > 0 and execution_status.status.conditions[0].type == "Failed"}
            next: execution_failed
          - condition: ${iteration >= max_iterations}
            next: execution_timeout

    - log_execution_progress:
        call: sys.log
        args:
          data: '${"üîÑ Execution " + execution_name + " still running... (check " + string(iteration + 1) + "/" + string(max_iterations) + ")"}'
          severity: INFO

    - wait_and_retry:
        call: sys.sleep
        args:
          seconds: ${poll_interval_seconds}

    - increment_counter:
        assign:
          - iteration: ${iteration + 1}
        next: poll_execution_status

    - execution_completed:
        steps:
          - log_completion:
              call: sys.log
              args:
                data: '${"‚úÖ Execution " + execution_name + " completed successfully"}'
                severity: INFO
          - return_success:
              return: ${execution_status}

    - execution_failed:
        steps:
          - log_failure:
              call: sys.log
              args:
                data: '${"‚ùå Execution " + execution_name + " failed: " + execution_status.status.conditions[0].message}'
                severity: ERROR
          - raise_error:
              raise: '${"Execution failed: " + execution_status.status.conditions[0].message}'

    - execution_timeout:
        steps:
          - log_timeout:
              call: sys.log
              args:
                data: '${"‚è∞ Execution " + execution_name + " timed out after " + string(timeout_minutes) + " minutes"}'
                severity: ERROR
          - raise_timeout:
              raise: '${"Execution timed out after " + string(timeout_minutes) + " minutes"}'

# Legacy subworkflow for job definition polling (kept for compatibility)
wait_for_job_completion:
  params: [job_name, project_id, timeout_minutes]
  steps:
    - init_polling:
        assign:
          - poll_interval_seconds: 30
          - max_iterations: ${timeout_minutes * 2}  # 30s intervals
          - iteration: 0

    - poll_job_status:
        call: googleapis.run.v1.namespaces.jobs.get
        args:
          name: '${"namespaces/" + project_id + "/jobs/" + job_name}'
          location: us-central1
        result: job_status

    - check_job_completion:
        switch:
          - condition: ${len(job_status.status.conditions) > 0 and job_status.status.conditions[0].type == "Complete"}
            next: job_completed
          - condition: ${len(job_status.status.conditions) > 0 and job_status.status.conditions[0].type == "Failed"}
            next: job_failed
          - condition: ${iteration >= max_iterations}
            next: job_timeout

    - log_job_progress:
        call: sys.log
        args:
          data: '${"üîÑ Job " + job_name + " still running... (check " + string(iteration + 1) + "/" + string(max_iterations) + ")"}'
          severity: INFO

    - wait_and_retry:
        call: sys.sleep
        args:
          seconds: ${poll_interval_seconds}

    - increment_counter:
        assign:
          - iteration: ${iteration + 1}
        next: poll_job_status

    - job_completed:
        steps:
          - log_completion:
              call: sys.log
              args:
                data: '${"‚úÖ Job " + job_name + " completed successfully"}'
                severity: INFO
          - return_success:
              return: ${job_status}

    - job_failed:
        steps:
          - log_failure:
              call: sys.log
              args:
                data: '${"‚ùå Job " + job_name + " failed: " + job_status.status.conditions[0].message}'
                severity: ERROR
          - raise_error:
              raise: '${"Job failed: " + job_status.status.conditions[0].message}'

    - job_timeout:
        steps:
          - log_timeout:
              call: sys.log
              args:
                data: '${"‚è∞ Job " + job_name + " timed out after " + string(timeout_minutes) + " minutes"}'
                severity: ERROR
          - raise_timeout:
              raise: '${"Job timed out after " + string(timeout_minutes) + " minutes"}'

# Simplified format data step (could be converted to another job later)
format_data_step:
  params: [game_id, project_id]
  steps:
    - log_format_start:
        call: sys.log
        args:
          data: '${"üìù Formatting training data for game: " + game_id}'
          severity: INFO

    # For now, we'll simulate this step
    # In a real implementation, this could be another Cloud Run Job
    # or call the format_training_data script via Cloud Build
    
    - simulate_formatting:
        call: sys.sleep
        args:
          seconds: 5

    - log_format_complete:
        call: sys.log
        args:
          data: '${"‚úÖ Training data formatting completed (simulated)"}'
          severity: INFO

    - return_format_result:
        return:
          success: true
          message: "Training data formatted successfully"

# Subworkflow to monitor Vertex AI tuning job completion
monitor_tuning_job:
  params: [job_name, project_id, region, timeout_minutes]
  steps:
    - init_polling:
        assign:
          - poll_interval_seconds: 60  # Check every minute for tuning jobs
          - max_iterations: ${timeout_minutes}  # 1-minute intervals
          - iteration: 0

    - poll_tuning_status:
        call: http.get
        args:
          url: '${"https://" + region + "-aiplatform.googleapis.com/v1/" + job_name}'
          auth:
            type: OAuth2
        result: tuning_status

    - check_tuning_completion:
        switch:
          - condition: ${tuning_status.body.state == "JOB_STATE_SUCCEEDED"}
            next: tuning_completed
          - condition: ${tuning_status.body.state == "JOB_STATE_FAILED"}
            next: tuning_failed
          - condition: ${tuning_status.body.state == "JOB_STATE_CANCELLED"}
            next: tuning_cancelled
          - condition: ${iteration >= max_iterations}
            next: tuning_timeout

    - log_tuning_progress:
        call: sys.log
        args:
          data: '${"üîÑ Tuning job " + job_name + " state: " + tuning_status.body.state + " (check " + string(iteration + 1) + "/" + string(max_iterations) + ")"}'
          severity: INFO

    - wait_and_retry:
        call: sys.sleep
        args:
          seconds: ${poll_interval_seconds}

    - increment_counter:
        assign:
          - iteration: ${iteration + 1}
        next: poll_tuning_status

    - tuning_completed:
        steps:
          - log_completion:
              call: sys.log
              args:
                data: '${"‚úÖ Tuning job " + job_name + " completed successfully"}'
                severity: INFO
          - log_model_info:
              call: sys.log
              args:
                data: '${"üéØ Fine-tuned model: " + tuning_status.body.tunedModel.name}'
                severity: INFO
          - return_success:
              return: 
                success: true
                state: ${tuning_status.body.state}
                tuned_model: ${tuning_status.body.tunedModel.name}
                tuning_job: ${job_name}

    - tuning_failed:
        steps:
          - log_failure:
              call: sys.log
              args:
                data: '${"‚ùå Tuning job " + job_name + " failed: " + tuning_status.body.error.message}'
                severity: ERROR
          - raise_failure:
              raise: '${"Tuning job failed: " + tuning_status.body.error.message}'

    - tuning_cancelled:
        steps:
          - log_cancellation:
              call: sys.log
              args:
                data: '${"‚ö†Ô∏è Tuning job " + job_name + " was cancelled"}'
                severity: WARNING
          - raise_cancellation:
              raise: '${"Tuning job was cancelled: " + job_name}'

    - tuning_timeout:
        steps:
          - log_timeout:
              call: sys.log
              args:
                data: '${"‚è∞ Tuning job " + job_name + " timed out after " + string(timeout_minutes) + " minutes"}'
                severity: ERROR
          - raise_timeout:
              raise: '${"Tuning job timed out after " + string(timeout_minutes) + " minutes"}'